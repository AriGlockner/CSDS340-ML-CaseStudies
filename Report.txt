Ari Glockner and Juan Martin LopezCase Study 1 ReportFinal Choice of Algorithm:We chose to use Gradient Boosting because it got the highest AUC (0.9182387216122225) and TPR (0.49839228295819976) scores in our tests. We used Scikit-learn’s Gradient Boosting Classifier and used the log loss function as the loss function. The rest of the parameters we left as the default parameters.Pre-processing data:Our pre-processing data consisted of shuffling our data and replacing all missing data with the average for that specific feature. We shuffled our data to ensure that we would get a random sample with minimal bias to make our algorithms more accurate. For filling in the missing data (-1’s in the csv file), we chose to fill in those data spots by choosing the average for the feature because we could avoid data loss and maintain data distribution while keeping our implementation simple. Process of Selecting our algorithm:In class we heard that going with an ensemble algorithm was probably the best way to go. This provided a good baseline for us for where to begin our search for the optimal algorithm. This led us to look at the following classifiers:* Random Forest (criteria: gini, entropy, log loss)* Bagging* Extra Trees* AdaBoost (various learning rates)* Gradient Boosting (loss: log loss, exponential)As well as a couple other algorithms we thought could work well:* Logistic Regression* SVMOur first step in selecting an algorithm was creating a baseline to be able to test multiple algorithms super easily. We did this by creating a list of each of our classifiers (and hyperparameters) and labels for our classifiers so that we could iterate through each classifier and test out each algorithm efficiently without having to modify much between each test.Once our baseline was created, we could start testing our algorithms using the AUC and TPR at FPR=0.01 as our scoring metrics. While we heard that ensemble learning algorithms were probably the best way to go, we decided we would test if that was true by testing out other algorithms like Logistic Regression and a few SVMs. Even after tuning our hyperparameters, our tests quickly confirmed that these algorithms weren’t as effective as the ensemble algorithms, so we eliminated these algorithms from further testing.After we had narrowed down our classifiers we wanted to move forward with, we needed to figure out which algorithm would be the best for any dataset.  After a few tests, we saw that there wasn’t always a single best classifier due to the randomization in some algorithms. While we could fix this by setting a random seed for testing our algorithm with our dataset, that would only help us determine the best algorithm for our dataset, not every dataset. Therefore, we decided to take an average of our AUC and TPR at FPR=0.01 scores to make it that our algorithm could be the best algorithm in most scenarios. When we ran our algorithms over 100 iterations, we found that the best algorithm (for both AUC and TPR) was Gradient Boosting using a criterion of Friedman MSE, and a loss of log loss. Since this algorithm was the best for our tests using 100 iterations, we believe it will perform well for the dataset that we don’t have access to. The full results of this test will be in the appendix.Recommendations on how to evaluate the effectiveness of our algorithm in practical use:In practical purposes, it would be worse if an important email gets deleted accidently then if a spam email is received by the person in the inbox. Therefore, to ensure that the algorithm does not filter out important emails, a good choice of metric for the algorithm is to have a larger penalty bias associated with deleting an important email. Soliciting feedback from users would then be determined first by does the spam bot delete something accidently and then pick the bot that lets in the fewest number of spam emails after the other condition is satisfied.Appendix:Average results from each classifier with 100 iterations: Classifier: Random Forest - giniAverage test set AUC: 0.9040659312555435Average TPR at FPR = 0.01: 0.39713826366559546 Classifier: Random Forest - entropyAverage test set AUC: 0.9088111871717929Average TPR at FPR = 0.01: 0.3658520900321539 Classifier: Random Forest - log lossAverage test set AUC: 0.9088111871717929Average TPR at FPR = 0.01: 0.3658520900321539 Classifier: Bagging/BootstrapAverage test set AUC: 0.8684724092955064Average TPR at FPR = 0.01: 0.16056672025723462 Classifier: Extra TreesAverage test set AUC: 0.9040486684171043Average TPR at FPR = 0.01: 0.4455305466237936 Classifier: Adaboost - 0.1 learning rateAverage test set AUC: 0.9072525233581114Average TPR at FPR = 0.01: 0.480353697749195 Classifier: AdaBoostAverage test set AUC: 0.8880943957580313Average TPR at FPR = 0.01: 0.23472668810289343 Classifier: Gradient Boosting - log loss, friedman_mseAverage test set AUC: 0.9182387216122225Average TPR at FPR = 0.01: 0.49839228295819976 Classifier: Gradient Boosting - exponential, friedman_mseAverage test set AUC: 0.9160367506649396Average TPR at FPR = 0.01: 0.4533762057877817 Best AUC: 0.9182387216122225 for Gradient Boosting - log loss, friedman_mseBest TPR: 0.49839228295819976 for Gradient Boosting - log loss, friedman_mse